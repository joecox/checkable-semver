\section{Introduction}

Software development in the internet age has become highly
modularized, where a project typically contains many modules developed
by independent teams, each with their own priorities and release
schedules. Managing so many dependencies has become quite
complicated. Tools like git and github have evolved to support
distributed development over the internet. Package managers like {\tt
  cpan}~\cite{cpan} for Perl, {\tt npm}~\cite{npm} for Node/JS, and
{\tt Maven}~\cite{maven} for Java tackle logistical problems --
distributing upgrades of a module to its users. A deeper problem
remains: upgraded packages can introduce subtle changes that might
introduce bugs in their clients. Often, these breaking changes might
lurk undetected for some time, and can be quite difficult to debug. To
guard against this, many developers {\em pin} their dependencies to a
particular version or version range, which instructs their package
manager to use only those versions that have been explicitly
specified. Pinning is problematic for several reasons.
%
First, it can block important bugfixes and improvements that are
perfectly compatible with the client code.
%
Worse, in systems containing many independently-maintained modules,
this can lead to ``dependency hell''~\cite{hell}, in which two modules
depend on incompatible versions of a third module.

%How does SemVer try to solve the problem?
%\begin{itemize}
%\item Establish a convention for version numbers that summarizes
%  the possible differences between two versions.
%\item SemVer is current state of the practice
%\end{itemize}

Semantic Versioning (SemVer)~\cite{semver} is a version numbering
convention that tries to solve these problems by giving a precise
meaning to each version number. Given two releases, the SemVer version
numbers summarize the possible differences between the two
releases. SemVer has been widely adopted, and is now considered the
current state of the practice.
%
%Where does SemVer fall short? 
%\begin{itemize}
%\item Lack of clear specification. 
%\item Types: too imprecise. Can't describe API behaviors.
%\item Documentation: not machine readable/checkable; can go stale; no tool support.
%\item Lack of tool support. Developers go by intuition. Error prone.
%\item Dependency hell is still common.
%\end{itemize}
%
However, there are several shortcomings with SemVer as it is used
today.
%
First, developers choose version numbers for a new release primarily
based on intuition, which can be error-prone. What they consider to be
a bugfix might actually change the behavior that their clients rely
on.
%
A second problem can be seen as a consequence of the first. Clients
are often suspicious of upgrades to some or all of their dependencies,
and put strict constraints on them to prevent their package manager
from installing untested dependencies. 
%
The root cause of these problems is an informal and often implicit
descriptions of what ``the interface'' is, i.e. what the clients can
depend over time.

% Problem setup / Motivation
It has always been difficult to rigorously specify library interfaces.
Interfaces are often defined in documentation or, for some programming
languages, defined with types. 
%
Documentation is often incomplete and not machine checkable. With no
validity checks there is nothing to prevent document specifications from
going stale or not follow the implementation at all. 
%
Types, on the other hand, are only as expressive as the type checker
allows. Consequentially, most type based specifications are incomplete. 
This leads to ambiguity and ambiguity leads to losses; either in the terms of
wrong assumptions made by the client, or some features which go unused. 
%
These problems are only taking one version of the interface into
account. When looking at the evolution of an interface, it is harder
still to ensure that the interface does not introduce a breaking change
unexpectedly. 

% Motivating examples
As an example, Sass is a widely used style language which compiles to
Css. The project contains of two side running targets Sass and
LibSass.  LibSass is a faster C++ implementation of Sass, and is
supposed to conform to Sass. In a minor version update (3.5), LibSass
rolled back a feature, which they felt safe doing because it was not
supported in Sass. In reality, this was a breaking change made in a
wrong version update. This situation would not have happened if the
interface was testable, as LibSass would have been forced to adhere
with the same specification as Sass~\cite{libsass}.
%


%{\bf What are our contributions?}
%\begin{itemize}
%\item A tool that implements CVT. Can check compliance or recommend
%  semver-compliant version numbers for new releases
%\end{itemize}

We address these problems by establishing a new testing discipline for
specifying module interfaces using tests. Tests are expressive enough
to specify behavioral contracts that are difficult or impossible to
express in traditional specifications languages or type systems. They
can also be used as the basis for tools that can automatically verify
SemVer compliance, or recommend version numbers for new releases.

Specification languages have been proposed for a variety of problem
domains and languages~\cite{uml}, though they have not achieved
widespread usage.
%
Some of the reasons for this lack of adoption include the steep
learning curve of the specification language, the often limited
expressiveness, and lack of good tool support.
%
On the other hand, testing disciplines such as unit testing,
regression testing, and integration testing have seen widespread
adoption in recent years.
%
Test-based interface specification complements existing testing
discipline, and does not require developers to learn a new
language. Our discipline is lightweight and can work with any testing
framework.

We supplement our testing discipline with a novel \emph{Cross-Version
Testing} approach to ensure that each release satisfies the
specifications of a range of releases, according to the rules of
Semantic Versioning. In particular, we test each release against the
interface test suites of certain {\em other} releases. According to
our specification scheme, any failing tests represent violations of
Semantic Versioning.

% Motivation of solution

% Contribution
% The goal of this paper is to solve the problem of incoherent and
% incomplete interfaces. To that end we have defined an interface specification
% as the collection of tests that only tests the interface of the library. We have
% shown that we can give valuable guarantees, both to the user of the library and
% the library developers them selves, not only in one version of the library but 
% over a history of releases. To give a direct application of the interfaces 
% specifications, we have created a tool that given a history of releases can
% predict if a release contains SemVer violations. 
% \todo{Describe the results of evaluation}

%{\bf How do we evaluate our contributions?}
%\begin{itemize}
%\item Subject program: Mocha. Developers identify a suite a API tests.
%\item Multi-faceted evaluation.
%\item detects semver violations.
%\item Infer a sequence of semver-compliant version numbers for Mocha's
%  releases.
%\item User study to evaluate practicality and limitations of our
%  testing discipline and approach
%\end{itemize}

We have evaluated our techniques by implementing Cross-Version Testing
in a tool, and conducting several experiments. 
%
Our tool works for Node/JS programs that use the node package manager
{\tt npm}.
%
We conducted experiments using the Mocha testing framework as our
subject library. We chose Mocha because it is a mature library with
many versions, it uses Semantic Versioning, and its developers follow
a principled testing discipline that is useful for our evaluation.
%
We first evaluate the quality of their interface tests based on a {\em
  CVT failure density hypothesis} that cross-version test failures
occur less frequently in interface tests than in all tests. This gives
us some confidence in using their ``interface tests'' as
specifications, even though they do not follow our discipline.
%
Using these tests as an interface specification, we detect SemVer
violations throughout the evolutionary history of Mocha.
%
We also infer a sequence of SemVer-compliant version numbers for
Mocha's releases, and compare with the version numbers chosen by
Mocha's developers.
%
Finally, we evaluate the difficulty of adopting our testing discipline
in Mocha, by manually inspecting several violations and refactors the
failing tests to conform to our discipline.

% Layout of the report. 
The rest of this report is as follows: in Section 2 we discuss related
work; in Section 3 we describe our approach; in Section 4 we discuss
our multi-faceted evaluation; in Section 5 we present our results; and
in Section 6 we conclude and discuss future work.

%\begin{itemize}
%\item in Section 2 we discuss related work;
% that covers the state of
%  the art in in terms of interface specifications, and version history
%  mining;
%
%\item in Section 3 we describe our approach; 
%an approach section,
%  which defines how to use tests to specify interfaces and how to use
%  these interfaces to detect SemVer violations;
%
%\item in Section 4 we discuss our multi-faceted evaluation;, which
%  describes the empirical studies we have performed to address some of
%  confounding factors;
%
%\item the results section, which contains the results of the studies
%  and a overall discussion of confounding factors;
%
%\item and lastly a conclusion, wrapping up the paper and presenting
%  the lessons learned.
%\end{itemize}

%We create a tool that can identify SemVer violations given a code
%repository, its testing framework, and a version history.  Our tool will apply
%tests appearing in each version of a repository to later versions of the code.
%Tests that succeed in an earlier version but fail in a later version are
%candidates for SemVer violations.  We will manually identify tests that are
%interface tests and the tool will further compare test behavior against
%versioning.  For example, if an interface test succeeds in an earlier version
%but fails in a subsequent minor update, then SemVer has been violated.  
%
%Thus, a library author can use our tool to detect potential SemVer violations
%by testing previous versions tests against the code the author is about to
%commit. Our tool will detect any SemVer violations and give the author the
%choice to either revert the API changes or push a major update.

