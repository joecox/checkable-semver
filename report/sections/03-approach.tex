\section{Approach}

Our approach consists of defining an interface specification, which is a
special kind of test, a new discipline for writing these tests, and a tool
that is able to detect SemVer violations in a library.

\subsection{Interface Specifications}

\begin{definition}\textit{Interface Specification.}
An interface specification is a collection of tests that, given an
implementation version, returns a list of interface violations. 
$$\text{Spec} : \text{Impl} \to \powerset{\text{Violation}}$$
A violation is a identifier of a failed test. A specification
\begin{enumerate}
\item Does not test any API secrets \cite{Parnas}, i.e.\ behaviors
  that are subject to change.
    \item Does test everything that a client can rely on.  
\end{enumerate}
\end{definition}

The interface is a two sided contract. On one side, the library
clearly specifies what can be relied upon. On the other side, clients
should not rely upon anything that is not specified in the interface
tests.  Doing so would tightly couple the client to a particular
library version, making upgrading the library unsafe. Our interface
specification is both sound and complete by definition.  Following the
definition, we have created a discipline of interface specification
using tests. We use the RFC 2119\footnote{The key words ``MUST'',
  ``MUST NOT'', ``REQUIRED'', ``SHALL'', ``SHALL NOT'', ``SHOULD'',
  ``SHOULD NOT'', ``RECOMMENDED'', ``MAY'', and ``OPTIONAL'' in this
  document are to be interpreted as described in RFC
  2119\cite{rfc2119}.} standard to describe the discipline.

\begin{enumerate}
    \item The library MUST clearly separate implementation specifications from
    other tests.
    \item Any released library version MUST be tested against the corresponding
    specification, and have no interface violations.
    \item Any feature exposed by the library, but not tested, SHOULD be
    considered a secret, and not be used by a client.
    \item A client MAY rely on any feature tested by the specification.
\end{enumerate}

This definition and discipline is flexible enough that it can describe
an interface, while still being machine-checkable.

\begin{figure*}
\begin{lstlisting}[language=javascript]
describe('Runner', function(){
  var suite, runner;

  describe('.globals()', function(){
    it('should default to the known globals', function(){
      runner.globals().length.should.be.above(10);
    })

    it('should white-list globals', function(){
      runner.globals(['foo', 'bar']);
      runner.globals().should.include('foo');
      runner.globals().should.include('bar');
    })
  })

  ...
})
\end{lstlisting}

\caption{Example API tests from mocha.js}
\label{fig:spec-example}
\end{figure*}

\subsection{Semantic Versioning} \label{sec:semver}

To show that the technique is useful in practice, we have chosen to implement
semantic versioning based of our interface specifications.

\begin{definition}\textit{Semantic Versioning} (SemVer) is a way of describing
intention, and giving guarantees using version numbers\cite{semver}.
Related to to the last version $X.Y.Z$, the next version has to
monotonically increasing, meaning that it has to satisfy one of the
following rules.
$$ \text{Version} : \mathbb{N} \times \mathbb{N} \times \mathbb{N} $$

\begin{enumerate}
    \item A patch release has number $X.Y.Z'$, where $Z < Z'$. It may include
    bug fixes only, no new features or breaking changes.
    
    \item A minor release has number $X.Y'.Z'$, where $Y < Y'$. It may include
    bug fixes or new features, but no breaking changes.
    
    \item A major release has number $X'.Y'.Z'$, where $X < X'$. It may include
    bug fixes, new features, and breaking changes.
    
    \item Any releases in the range $0.Y'.Z'$, where $X = 0$ and $Y < Y'$ or $Z
    < Z'$ is unstable and the interface can change without warning.
\end{enumerate}

\end{definition}

We have chosen semantic versioning because it is a widely used
convention for managing dependencies that evolve independently of
their clients.
%
It is also encouraged and supported by package managers like
\emph{npm}, and \emph{bower}.
%
\marktodo{Both package managers depend heavily on that dependencies
  follow the SemVer policy}{check this. Matt's unsure about this.}.
%
Semantic version numbers summarize the possible differences between
releases of a library, in particular, the possible differences visible
to a client. For example, a patch release can fix some buggy
behaviors, but must not add new behavior or break any old behavior. A
minor release can fix bugs and add new behavior, but must not break
old behavior.

\subsection{Cross-Version Testing} \label{sec:cvt}
To verify compliance to semantic versioning, we examine a series of
releases.
%
We define a release as a tuple of a version number, an implementation
and a specification.
$$ \text{Release} : \text{Ver} \times \text{Impl} \times \text{Spec} $$
%
Our \emph{Cross-Version Testing} approach ensures that each release
satisfies the specifications of particular other releases, as required
by the rules of Semantic Versioning. Given a release of interest and
the complete set of releases, our cross-version testing tool will
produce a set of implementations and specifications. A particular
implementation and specification are included in the output set if
SemVer requires that the implementation meet the specification, but
this is not the case.
$$
CVT : \text{Release} \times \powerset{\text{Release}} \to \powerset{\text{Spec} \times \text{Impl}}
$$

{\bf Why do we do this? Why is it a reasonable thing to do?}
Cross-version testing allows developers to refactor their interface
tests as needed, without introducing difficult problems surrounding
equivalence of specifications. \marktodo{Probably need to say more
  about this somewhere}{}.

{\bf The rules:} \marktodo{Below needs to be updated. It's assuming
  that the release is ``newer'' than the set of releases. In general,
  it will be (any) one of them}{}.

\begin{enumerate}
\item The implementation of a release must meet the specification of
  that release;
  $$ Refl(r) = \{(s, i)\} \text{ where } ((x,y,z), i, s) = r $$

    \item all previous implementations with versions in same minor version as the
    new release, must meet new specification without violations (Added Feature
    Test);
    $$AFT(r, rs) = \left\{ (s, i')\ \vline\ 
        \begin{matrix}
            r' \in rs \\
        ((x,y,z), \_, s) = r \\ 
        ((x,y,z'), i', \_) = r'\\
        z' < z \\
        \end{matrix}
    \right\}$$

    \item and the new implementation must meet all previous specifications in the same major version as the new
        release, without violations (Breaking Change Test).
    $$BCT(r, rs) = \left\{ (s', i)\ \vline\ 
        \begin{matrix}
            r' \in rs \\
        ((x, y, z), i, \_) = r \\ 
        ((x, y', z'), \_, s') = r'\\
        y' < y \\
        \end{matrix}
    \right\}$$
\end{enumerate}

If all the steps are passed, the release is following SemVer. Assuming that the 
second point is uphold, we can write down the expression for cross version testing. 
$$ CVT(r, rs) = Refl(r) \cup AFT(r, rs) \cup BCT(r, rs) $$

\begin{theorem}[Soundness]
Given a release and a release history, our algorithm will reject the release if
and if it violates SemVer in relation to the release history.
\end{theorem}

\begin{proof}[sketch]
If a release is rejected, it must have failed in one of the 3 conditions. 
\begin{itemize}
\item If the first condition failed, the release is invalid, and semantic
  versioning does not apply to broken releases. 
\item If the second requirement failed, then the implementation of
  some previous release with the same major and minor version does
  not meet the specication of the current version. This means that
  the current version specification includes either a new feature
  or a breaking change, which violates the first rule of SemVer.
\item If the third requirement failed, some previous release, in the same
  major version,  must have a spec which is violated by the current
  implementation. This means that something which was guaranteed has been
  broken; which makes this a breaking change. Since the new release is at
  most a minor release compared, this violates the second rule of SemVer.
\end{itemize}
Because all possible failed cases is the result of a SemVer violation, the
algorithm is guaranteed to only reject a release if SemVer is violated.
\end{proof}

\begin{theorem}[Completness]
Given a release and a release history, our algorithm will accept the release if
and if adheed to SemVer in relation to the release history.
\end{theorem}

\begin{proof}[sketch]
If a release is wrongly accepted it must have not satisfied any of the SemVer
rules. 

\begin{itemize}
    \item If the new release is a major release, there is no way to violate the
    SemVer rules. If the new release is a minor release, it must contain a breaking
    change to violate SemVer. 
    \item If the new release is a minor release it would have been checked against all
    previous specifications in the major version, and passed without violations.
    This means that in respect to any previous version there is no breaking
    changes. So the new release does not violate SemVer.\
    \item If the new release is a patch release it would both have been checked against all 
    previous specifications and run the current new specification on each
    implementation with the same minor version. Nothing new has been added to the
    specification, so no (documented) features has been added. This release has not
    violated SemVer.
\end{itemize}

Because any version release accepted, whether it is patch, minor, major will all be according to 
SemVer, our tool can not accept a wrong release.
\end{proof}

\subsection{Pragmatics}
What is required of library authors that want to adopt our approach?
Here we discuss some high-level requirements that should be followed.

\begin{itemize}
\item A library should upper-bound their dependencies to stay within
  the current major version, since updrading to the next major version
  of a dependency may introduce a breaking change, which in turn could
  cause a breaking change for the library itself. We found this
  problem with mocha, and had to modify its {\tt package.json} file to
  correct it.
\item NPM packages specify a set of {\tt devDependencies} in the {\tt
    package.json}, which are the dependencies needed by the test suite
  but not the library itself. When cross-version testing, it is
  important to use the {\tt devDependencies} of the tests, not those
  of the library. Therefore, we found that we had to modify {\tt
    package.json} of the library each time we ran a cross-version
  test.
\end{itemize}

\subsection{Violation Density Hypothesis}
We hypothesize that the density of violations, defined as the average
number of violations per test, will be lower when cross-verstion
testing the {\tt test-jsapi} suite than when cross-version testing the
{\tt test-all} suite. We expect this to be true because even though
the mocha developers don't follow our testing discipline, they do
follow SemVer and identify the {\tt test-jsapi} test suite as their
interface tests. Therefore, we expect {\tt test-jsapi} to be more
consistent across versions than average. If the density of violations
is much lower for {\tt test-jsapi} than for {\tt test-all}, it will
give us some confidence that {\tt test-jsapi} is useful as a SemVer
specification.

We evaluated the violation density hypothesis by computing the
percentage of test cases that failed during cross-version testing of
each of the two test suites, {\tt test-jsapi} and {\tt test-all}.  We
identify several confounding factors with this approach:
\begin{itemize}
\item Manual inspection showed that {\tt test-jsapi} contains unit
  (secret) tests. There may be others, since we only inspected failing
  test cases of the version 1.0.0 {\tt test-jsapi} suite. This could
  skew the results. Still, our density hypothesis should be robust
  against this, since the hypothesis is that there are fewer secret
  tests in {\tt test-jsapi} than average, not that there are {\em no}
  secret tests in {\tt test-jsapi}.
\item Cross-version testing with {\tt test-all} sometimes crashes, so
  not all tests are run. This means we may be measuring too few
  failing in {\tt test-all} tests, which can skew the average {\tt
    test-all} failures downward.
\item Early versions of mocha depend on an old (incompatible) version
  of node.js. The incompatibility means that we cannot run {\tt
    test-all} at all for these versions, though {\tt test-jsapi} still
  works. In these cases, we under-count the number of cross-version
  {\tt test-all} tests that should be run for a version. In extreme
  cases, fewer {\tt test-all} tests are run than {\tt test-jsapi}
  tests, though {\tt test-all} by definition includes {\tt
    test-jsapi}. This can skew the average {\tt test-jsapi} failures
  upward, so we filter such cases out.
\end{itemize}

Despite these confounding factors, we found that {\tt test-all}
contains 46\% more failures than {\tt test-jsapi}, which indicates
that even though mocha developers do not follow our discipline, the
{\tt test-jsapi} is {\em more} compliant with our discipline than all
tests, indicating that there is some validity in using it as an
interface specification.  Further work is required to evaluate how
effective failure density can be to bootstrap interface specs for
projects that want to adopt our discipline.

\subsection{Tool Implementation}
To support the approach we create two tools: {\tt detect} and {\tt
  simulate}. The {\tt detect} tool is given a version tag and
produces SemVer violations associated with trying to bump to that
tag. The {\tt simulate} tool is given a version tag and simulates a
version history starting at that tag by using our approach.

The purpose of the {\tt detect} tool is to determine how many SemVer
violations the Mocha authors committed by publishing each version. To
that end, the tool takes a version tag, such as {\tt 1.0.1} or {\tt
  2.3.0}, as an argument and calls a {\tt cvt} module to compute the
violations.

The {\tt cvt} module, given a version tag and a version history,
generates a set of pairs of implementation versions and test
versions. For each pair, {\tt cvt} will checkout the specified
implementation, checkout the specified test suite, and run the test
suite. Some of the pairs represent Added Feature Tests and some
represent Breaking Change Tests.

\begin{itemize}
\item For those that represent Added Feature Tests, the test version
  is the given version tag, and the implementation version is a
  previous tag within the minor version. For example, running test
  version {\tt 2.3.4} against implementation version {\tt 2.3.0} is
  testing for added features. Failures in the test suite mean that
  features have been added since the last minor version update.
\item For those that represent Breaking Change Tests, the
  implementation version is the given version tag, and the test
  version is a previous tag within the major version. For example,
  running test version {\tt 2.0.0} against implementation version
  {\tt 2.3.4} is testing for breaking changes. Failures in the test
  suite mean that there has been a breaking change in the interface
  since the last major version update.
\end{itemize}

The purpose of the {\tt simulate} tool is to test how our approach
would fare if used in a real library. The {\tt simulate} tool
generates a simulated version history. Given a starting version, the
tool will iteratively propose new SemVer tags by running Added
Feature tests and Breaking Change tests against the growing simulated
version history. The tests are run using a modified version of the
{\tt cvt} module. The {\tt cvt} module determines which Added Feature
and Breaking Change tests to run from the simulated version history,
but must map the simulated tags back to the actual revisions they
represent in the repository, else the module may check out an
incorrect version.

