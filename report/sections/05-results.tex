\section{Results}

This is section is a discussion and presentation of the results of our
evaluation.

\subsection{Quantitative Analysis}

\begin{figure*}
\centering
\includegraphics[height=50ex]{graphics/combined-violations-plt}
% \includegraphics[height=50ex]{graphics/violations-unit-plt}
% \includegraphics[height=50ex]{graphics/violations-all-plt}
\caption{Violations per version. {\tt jsapi} is a subset of violations found in {\tt all}}
\label{fig:violations}
\end{figure*}

{\bf RQ1.} {\em Does our tool detect SemVer violations?}
Figure~\ref{fig:violations} shows the number of violations found in
each version of Mocha.  We classify each violation as either a
breaking change in a minor version release, or a new feature in a
patch version relase. We also compare the number of violations found
in the {\tt jsapi} suite (shown in blue and green) versus the {\tt all}
suite (orange and yellow), of which {\tt jsapi} is a subset. Note that
cross-version testing will run multiple versions each test suite. We
count the tests in each version separately.

The figure shows the evolution of mocha over the releases tagged with
versions from 1.0.0 and up. The orange and blue columns are breaking
changes in {\tt all} and {\tt jsapi} respectively. A breaking change is when
a test in a prior release in the same major version fails. The yellow
and green columns are added features. An added feature is when an
prior implementation in the same minor version from fails on the new 
test. The {\tt jaspi} column is shown on top of the {\tt all} column to
safe space. This can be done, because {\tt jsapi} is a subset of the 
test run in {\tt all}.

The versions 1.3.0, 1.15.0 and 1.18.0 are especially interesting as they
introduce new breaking changes in respect to {\tt jsapi}.  The 1.18.0
increment is not a SemVer violation, the developers just tested secrets
in the {\tt jsapi}.  We discuss this in detail in
section~\ref{sec:failure1}.  The 1.3.0 increment is due to a single
violation described in section~\ref{sec:failure2}. The 1.15.0 increment
is mostly due to a violation is described section~\ref{sec:failure3}. It
is interesting that after the major version 2, almost no breaking
changes comes from {\tt jsapi}. This suggest that the interface tests
has matured, and are more stable. The lack of violations in version
v2.5.0 was caused by a problem with npm that we couldn't correct. The
steep increase in violations in version 1.21.5 might indicate that some
new features and breaking were snuck in before the upcoming major
release. Alternatively, these changes may have pushed them to release
the major version to correct these violations.

This analysis shows that even with tests that don't follow our
discipline, cross-version testing can provide insight into the
evolution of library.

\begin{figure*}
\centering
\includegraphics[height=50ex]{graphics/simulation-plt}
\caption{Simulation of version histories}%
\label{fig:simulation}
\end{figure*}

We used our tool to infer a correct version history based on Mocha's
{\tt jsapi} test suite. The results are shown in
Figure~\ref{fig:simulation}. The purple line shows the numbering
chosen by the developers, and the green line is the simulated
numbering inferred by our tool.  Our simulation ignored 3 of the 66
{\tt jsapi} tests that caused failures in 12 consecutive versions. We
diagnosed the cause of these failures to be a bad specification of
dependencies, similar to the problems we described in the
introduction.

The key knowledge we can draw from Figure~\ref{fig:simulation} is
that, even with this stricter sense of interface, we don't see the
major version explosion that critics~\cite{crawford-not-semver,
  backbone-2888, exoplayer-1382} of SemVer predict. In fact, the
simulation ended up only on major version 5, compared to the actual
major version 2. We contend that if the tests were written following
our discipline and the developers had access to our tool before making
releases, it would be easy to eliminate all violations and infer the
same numbering as used by devlopers. It is also interesting that even
though our tool infers a major version for the 6$^{th}$ release, it
recommends more patches from that release to the 30$^{th}$
release. This might be because the developers felt they have added
features, and therefore bumped the minor version
unnecessarily. Either that or they did not write a test documenting
the added features in {\tt jsapi}. From release 54 onward, our
simulated history almost perfectly mimics the developers
version. This suggests that at this point the test suite has
naturally evolved to follow our discipline. We find this to be an
encouraging indication that our discipline is reasonable.

\begin{figure*}
\centering
\includegraphics[height=50ex]{graphics/combined-cumulative-plt}
\caption{Cumulative names of violated tests} 
\label{fig:cumulative}
\end{figure*}

To address some of the confounding factors related to the library
itself, we collected the cumulative unique names of violated tests
for each version in Figure~\ref{fig:cumulative}. This accounts for the
fact that two tests with the same name in different versions might in
fact be the same.
%
Comparing Figure~\ref{fig:cumulative} with Figure~\ref{fig:violations}
shows that many of the violations found are due to the same test
failing multiple times during cross-version testing with multiple
versions.
%
After version 2.0.0 there is an explosion of unique failing tests in
the {\tt all} suite. 
%
This adds confirmation to the violation density hypothesis that the
{\tt all} suite contains more internal tests than {\tt jsapi}.
%
On the other hand, {\tt jsapi} levels off, showing that no new tests
are being violated.
%
The breaking change violations detected in versions 1.15.0, 1.18.1 and
1.21.0 are also clearly visible in this graph.

These figures yield nontrivial information about the stability of the
library that would not be possible without cross version testing. The
figures also show that our techniques are robust to some of the
confounding factors. Cross-version testing can detect SemVer
violations even without tests designed in accordance with our
discipline. Also, adhering to SemVer does not result in a major version
explosion, even when based of a partial and possible incorrect
specification.

\subsubsection{Confounding Factors}

The {\tt all} test suite runs some tests multiple times due to the
way it is setup. To get more accurate results, we could remove
duplicate test titles from total and failing tests, but on the other
hand, some distinct Mocha tests have the same titles, so when we
remove duplicate titles, we lose some of that information. 

Because Mocha has been developed for multiple years, and Node/JS itself
has changed between releases, this makes cross version testing harder
because some older releases might depend on older versions of Node/JS and
on older libraries. Some of the violations might therefore be a product
of a wrong version of Node/JS, and not an actual bug in the code. 

Because we test a testing framework on itself it is hard to clearly
separate the tests from the implementation. Some violations might be 
due to conflicting Mocha specifications and implementations and not
the test itself.

\subsection{User Study Results}
We used the test suite of Mocha as our subject library. The developers
identify the {\tt jsapi} test suite, comprising 66 tests, as their
interface tests. Some of these test methods are annotated
{\tt @api private}, which might indicate that these methods are not
part of the client-facing API\@. However, judging from the
documentation, it seems that these are treated more as advanced
features that clients may want to use in rare cases, but most of the
time can safely ignore. We decided to use all the {\tt jsapi} tests as the
interface spec because 1) it is unclear which of the {\tt @api
  private} methods are truly secrets, and 2) inferring which
method(s) an interface test is intended to describe is often difficult and
outside the scope of this work.

According to our discipline (assuming the Mocha developers had adopted
our discipline), the tests included with Mocha version 1.0.0 would
pass when run against every version in the 1.X.Y range. There are 50
versions in this range. We found that 9\% of the {\tt jsapi} tests from
version 1.0.0 (6 out of the 66) failed on subsequent versions.

Since Mocha developers are not using our discipline, we expect that some of
these failures are due to not following the discipline; the test
depends (either explicitly or implicitly) upon interface secrets. These
cases do not necessarily indicate SemVer violations, as long as
removing the dependence on interface secrets makes the tests pass. We
manually inspected each failing {\tt jsapi} test for interface secrets, and
refactored the test to remove the secret. The failure in
section~\ref{sec:failure1} below gives an example of this.

We manually inspected of the 6 failing {\tt jsapi} tests to identify the
cause of the failure and remove any interface secrets. We found that 4 of
the 6 test failures were multiple instances of a single problem, so
there were 3 types of failure. Of the 3 failure types, 2 failed due to
testing secrets as well as the interface, and the tests succeeded after
removing the exposed secrets. Altogether, the classification and
refactoring of these tests required 4 hours of manual effort of the
authors, who were unfamiliar with Mocha's internals. We did not find any
instances in which a test could not be refactored. This experience
demonstrates that that adopting our discipline may not be too difficult.

We summarize the 3 failing {\tt jsapi} tests, our diagnosis of the cause of
the failure (SemVer violation or exposed secret), and in the cases of
exposed secrets, the fix required for the test to conform to our
discipline.

\subsubsection{Failure 1: Interface test makes assertions about
interface secrets.}
{\bf Test description: } {\em Suite .beforeAll() wraps the passed in
function in a Hook adds it to \_beforeAll}
\label{sec:failure1}

There are three other tests that are very similar. Together, these
four tests describe the behavior of event hook registration
functions. The {\tt Suite} class signals events at different stages of
executing a test suite; before running any tests (beforeAll), after
all tests have been run (afterAll), just before each test is run
(beforeEach), and just after each test is run (afterEach). Each event
has its corresponding event hook registration method, and each
registration method has its corresponding test case. The test case
ensures that the correct metadata is created for each event hook, that
the events are emitted in the correct order, etc.

Part of the metadata for an event hook is a title used in Mocha's
output to the user. Each test asserts that the title is a particular
string.

{\small
\begin{verbatim}
beforeAllItem.title.should.equal('"before all" hook');
\end{verbatim}
}

Starting in version 1.15.0, Mocha added a feature that allows the test
writer to optionally give a name to the hook, which is then included
in the title. If a title is not given, the default is to use the name
of the hook function. The test fails because the new default behavior
is not consistent with the old behavior.

We judge this not to be a SemVer violation, but rather a violation of
our testing discipline. The title of the hook is not meant to be part
of the client interface, just part of the report
formatting. Therefore, our discipline dictates that such an assertion
should not be made in an interface test, and should instead be moved
to an internal test.

The problem is that the assertion is part of a complicated test
framework, that mostly tests the interface. To separate the
secret assertion from the interface tests, and in order to adopt our
discipline, the Mocha developers could duplicate the setup in two places,
first in the interface specification, and second in the
internal test suite. The internal test suite could then to check secrets
like that the title is constructed correctly. This would either require
some code duplication, or some refactoring of the test in order to reuse
code. There are the usual risks associated with each option: duplicating
code means that as the interface evolves, the same changes must be
applied in two places; refactoring test code can make it more complex,
which might limit its benefit as a specification to client developers.

\subsubsection{Failure 2: Error changes.}
\label{sec:failure2}
{\bf Test description: }
%
{\em Runnable(title, fn) .run(fn) when async when the callback is
  invoked several times with an error should emit a single "error"
  event }

\begin{figure*}
\begin{lstlisting}
describe('Runnable(title, fn)', function(){
  describe('.run(fn)', function(){
    describe('when async', function(){
      describe('when the callback is invoked several times', function(){
        describe('with an error', function(){
          it('should emit a single "error" event', function(done){
            var calls = 0;
            var errCalls = 0;

            var test = new Runnable('foo', function(done){
              done(new Error('fail'));
              process.nextTick(done);
              done(new Error('fail'));
              process.nextTick(done);
              process.nextTick(done);
            });

            test.on('error', function(err){
              ++errCalls;
              err.message.should.equal('done() called multiple times');
              calls.should.equal(1);
              errCalls.should.equal(1);
              done();
            });

            test.run(function(){
              ++calls;
            });
          })
        })
\end{lstlisting}
\caption{A failing mocha {\tt jsapi} test}
\label{fig:failing-test}
\end{figure*}

The test, shown in Figure~\ref{fig:failing-test}, defines a
part of the behavior of Mocha's asynchronous test runner. The test 
asserts that when two asynchronous error events are triggered, only one
is delivered to the error callback. This is important in tests that
allocate resources like database connections, which need to be released
exactly once.

In addition to testing that only a single error event is delivered,
the version 1.0.0 test also tests {\em which} error event is
delivered. In particular, line 20 tests that the error message is a
particular string. The behavior changes at version 1.3.0, from
delivering an error created by Mocha that indicates improper usage of
its API, to one of the errors passed by the client to the callback.

Is this case a breaking change, a new feature, or something else?
We might consider that the exceptions thrown by Mocha to its clients
are part of the interface, and so this should be considered a breaking
change. Because that the test author's intent is unclear, we follow the
test description, which mentions only the number of error events that
should be delivered. Therefore, we considered {\em which} error event
is delivered to be secret information, and removed it from the test
simply by deleting line 20.

\subsubsection{Failure 3: SemVer violation.}
\label{sec:failure3}
{\bf Test description:}
%
{\em Runner .failHook(hoot, err) should emit "end" }

The method {\tt Runner.failHook} calls a failure hook, and after that
hook has run, emits an {\tt end} event to allow test clean up to occur. In
version 1.21.5, whether the {\tt end} event should be emitted became
configurable by setting the property {\tt suite.bail}. The
description of this test was changed to reflect this:

{\em Runner .failHook(hoot, err) should emit "end" if suite bail is
  true }

The original test fails because the default value of suite.bail is
{\tt false}, so if a client upgrades Mocha to version 1.21.5, the
{\tt end} event will no longer be emitted. This is a breaking change in
a patch version, and thus is a SemVer violation. We note that the {\tt
  Runner.failHook} method is annotated {\tt @api private}, so this test
may not actually be part of the interface specification.

\subsubsection{Confounding Factors}
We identify three confounding factors with our user study. First, it
is still unclear that the {\tt jsapi} tests should be used as Mocha's
interface specification. These tests were identified by the Mocha
developers as the {\tt JavaScript API tests}, and among the JavaScript
libraries we considered for our subject program, seemed to be the most
reasonable test suite to use as a specification. We assume that what
the Mocha developers mean by {\tt API} is the interface between Mocha and
its client. However, this may not always be the case. In practice, this
test suite tests internal interfaces as well as the external
client-facing interface.

A second confounding factor is the small sample size of our study:
only refactored one test suite version of one library. The scope was
limited by the effort required to get good cross-version test
results. Given more time, we would like to examine and refactor more
cross-version testing failures from Mocha other libraries.

A third confounding factor is that the quality of Mocha's test suite
is likely to be quite good compared with the average JavaScript test
suite. This is to be expected, since Mocha itself is a test suite, so
its developers would be expected to be more {\tt pro-test} than the
average developer. For this reason, Mocha's test suite may not be
representative of other test suites.

