\section{Results}

This is section is a discussion and presentation of the result of the
evaluation.

\subsection{Quantitative Analysis}

\begin{figure*}
\centering
\includegraphics[height=50ex]{graphics/combined-violations-plt}
% \includegraphics[height=50ex]{graphics/violations-unit-plt}
% \includegraphics[height=50ex]{graphics/violations-all-plt}
\caption{Violations per version. ``jsapi'' is a subset of violations found in ``all''}
\label{fig:violations}
\end{figure*}

The first thing we wanted to show is that we could find violations using test.
Figure~\ref{fig:violations} is a plot of violations encounter in each version.
The ``jsapi'', blue and green, suite is a subset of the test run in ``all'', orange and
yellow. Since a SemVer violation is tied to an implementation version and test version, the same test
will be counted multiple times.

The versions 1.3.0, 1.15.0 and 1.18.0 are especially interesting as they
introduce new breaking changes in respect to ``jsapi''.  The 1.3.0 increment is
a single violation is analysed in depth in section~\ref{sec:failure2}. The
1.15.0 increment is mostly due the SemVer violation that we found in and
described in section~\ref{sec:failure3}. The 1.18.0 increment is not a
SemVer violation, the developers just tested secrets in the jsapi.
The violation has been described in detail in section~\ref{sec:failure1}. The
steep increase in breaking changes 1.21.5, could indicate that they were working
on creating the new major release, and some of the new features; green and
yellow, and some breaking changes might have sneaked in the last release in major
version. It is interesting that after the major version 2, almost no breaking
changes comes from ``jsapi''. This suggest that the interface tests has
matured, and are more stable. A funny anomaly happened in v2.5.0 where the npm
stopped working on that package.

This analysis shows that even with test not designed to be interface
specifications we can generate real and relevant information about a 
version history by doing cross version testing.

\begin{figure*}
\centering
\includegraphics[height=50ex]{graphics/simulation-plt}
\caption{Simulation of version histories}%
\label{fig:simulation}
\end{figure*}

Figure~\ref{fig:simulation} is a simulation of running our tool 
on the entire releases history of mocha. The purple line is the 
actual version numbers chosen by the developers, and the green line is the
version history chosen by our tool.  For the simulated version, we used the
``jsapi'' suite, and had 3 out of 66 test ignored,
which violated the interface in 12 consecutive versions, and was mostly do to
bad specification of dependencies. 

The key knowledge we can draw from this graph is that, even with this stricter
sense of interface, we don't see the major version explosion, that critics of
SemVer predicts. In fact the simulation ended up only on major version 5,
compared to the actual major version 2. We think that the tests if actually
written as specifications and the developers where warned about SemVer
violations, would have an even better major version history.
Also it is interesting that even though version is bumped a major version
already in release 6, the simulation is more liberal with patches from that 
release to release 30. This might be because the developers felt they have added 
features, and therefor bumped the minor version unnecessarily. Either that
or they did not write a test documenting the added features in ``jsapi''. From 
release 54 almost perfectly mimics the developers version. This is means that the
developers ,without their knowledge, have adhered to our discipline. 

\begin{figure*}
\centering
\includegraphics[height=50ex]{graphics/combined-cumulative-plt}
\caption{Cumulative names of violated tests} 
\label{fig:cumulative}
\end{figure*}

To address some of the confounding factors related to the library itself we
collected the cumulative unique names of the violated test for each version in
figure~\ref{fig:cumulative}. We take the cumulative unique names, grouping test
that changed or have the same name. Using this metric we under approximate see
the number of non interface tests in each of the targets. In contrast to
figure~\ref{fig:violations} this better show the separation of interface tests
and internal tests. After version 2.0.0 there is an explosion of violated tests
in the ``all'' suite, which have different names. This suggest that all
contains more internal tests than the ``jsapi''. ``jsapi'' on the other hand
seams to plateau, which means that there is no new tests, that is being
violated. The breaks in 1.15.0, 1.18.1 and 1.21.0 are also clearly visible in
this graph. 

We feel like these figures yield nontrivial information about the stability
of the library that would not be possible without cross version testing. The
figures also manage to challenge some of the confounding factors, surrounding
both SemVer and cross version testing. Cross version testing can detect SemVer
violations, even though it work better with desiccated tests. Also adhering to
SemVer does not result in a major version explosion, even when based of a 
partial and possible incorrect specification.

\subsubsection{Confounding Factors}

The ``all'' test suite runs many tests multiple times. Runs all categories,
individual tests are included in more than one category. We could remove
duplicate test titles from total and failing tests, but on the other
hand, some distinct mocha tests have the same titles, so when we remove
duplicate titles, we lose some of that information. There are fewer
duplicate titles than tests that are re-run though, so we choose the
thing that minimizes the error.

Because mocha has been developed for multiple years, that node itself
has changed between releases, this makes cross version testing harder
because some older releases might depend on older versions of node and
of older libraries. Some of the violations might therefor be a product
of a wrong version of node, and not an actual bug in the code. 

Because we test a testing framework with itself it is hard to clearly
separate the tests from the implementation. Some violations might be 
due to the combination of mocha specification and implementation and not
the test itself.

\subsection{User Study Results}
We used the test suite of mocha as our subject library. The developers
identify the ``jsapi'' test suite, comprising 66 tests, as their
``interface tests''. Some of these test methods that are annotated
{\tt @api private}, which might indicate that these methods are not
part of the client-facing API\@. However, judging from the
documentation, it seems that these are treated more as advanced
features that clients may want to use in rare cases, but most of the
time can safely ignore. We decided to use all the ``jsapi'' tests as the
interface spec because 1) it is unclear which of the {\tt @api
  private} methods are truly ``secrets'', and 2) inferring which
method(s) an interface test is intended to describe is often difficult and
outside the scope of this work.

According to our discipline (assuming the mocha developers had adopted
our discipline), the tests included with mocha version 1.0.0 would
pass when run against every version in the 1.X.Y range. There are 50
versions in this range. We found that 9\% of the jsapi tests from
version 1.0.0 (6 out of the 66) failed on subsequent versions.

Since mocha developers are not using our discipline, we expect some of
these failures to be due to not following the discipline; the test
depends (either explicitly or implicitly) upon interface secrets. These
cases do not necessarily indicate SemVer violations, as long as the
removing the dependence on interface secrets makes the tests pass. We
manually inspected each failing ``jsapi'' test for interface secrets, and
refactored the test to remove the secret. The failure in
section~\ref{sec:failure1} below gives an example of this.

We manually inspected of the 6 failing ``jsapi'' tests to identify the
cause of the failure and remove any interface secrets. We found that 4 of
the 6 test failures were multiple instances of a single problem, so
there were 3 types of failure. Of the 3 failure types, 2 failed due to
testing secrets as well as the interface, and the tests succeeded after
removing the exposed secrets. Altogether, the classification and
refactoring of these tests required 4 hours of manual effort of the
authors, who was unfamiliar with mocha's internals. We did not find any
instances in which a test could not be refactored. This experience
demonstrates that that adopting our discipline may not be too difficult.

We summarize the 3 failing ``jsapi'' tests, our diagnosis of the cause of
the failure (SemVer violation or exposed secret), and in the cases of
exposed secrets, the fix required for the test to conform to our
discipline.

\subsubsection{Failure 1: Interface test makes assertions about
interface secrets.}
{\bf Test description: } {\em Suite .beforeAll() wraps the passed in
function in a Hook adds it to \_beforeAll}
\label{sec:failure1}

There are three other tests that are very similar. Together, these
four tests describe the behavior of event hook registration
functions. The {\tt Suite} class signals events at different stages of
executing a test suite; before running any tests (beforeAll), after
all tests have been run (afterAll), just before each test is run
(beforeEach), and just after each test is run (afterEach). Each event
has its corresponding event hook registration method, and each
registration method has its corresponding test case. The test case
ensures that the correct metadata is created for each event hook, that
the events are emitted in the correct order, etc.

Part of the metadata for an event hook is a title used in mocha's
output to the user. Each test asserts that the title is a particular
string.

{\small
\begin{verbatim}
beforeAllItem.title.should.equal('"before all" hook');
\end{verbatim}
}

Starting in version 1.15.0, mocha added a feature that allows the test
writer to optionally give a name to the hook, which is then included
in the title. If a title is not given, the default is to use the name
of the hook function. The test fails because the new default behavior
is not consistent with the old behavior.

We judge this not to be a SemVer violation, but rather a violation of
our testing discipline. The title of the hook is not meant to be part
of the client interface, just part of the report
formatting. Therefore, our discipline dictates that such an assertion
should not be made in an interface test, and should instead be moved
to an internal test.

The problem is that the assertion is part of a complicated test
framework, that mostly tests the interface. To separate the
secret assertion from the interface tests, and in order to adopt our
discipline, the mocha developers could duplicate the setup in two places,
first in the interface specification, and second in the
internal test suite. The internal test suite could then to check secrets
like that the title is constructed correctly. This would either require
some code duplication, or some refactoring of the test in order to reuse
code. There are the usual risks associated with each option: duplicating
code means that as the interface evolves, the same changes must be
applied in two places; refactoring test code can make it more complex,
which might limit its benefit as a specification to client developers.

\subsubsection{Failure 2: Error changes.}
\label{sec:failure2}
{\bf Test description: }
%
{\em Runnable(title, fn) .run(fn) when async when the callback is
  invoked several times with an error should emit a single "error"
  event }

\begin{figure*}
\begin{lstlisting}
describe('Runnable(title, fn)', function(){
  describe('.run(fn)', function(){
    describe('when async', function(){
      describe('when the callback is invoked several times', function(){
        describe('with an error', function(){
          it('should emit a single "error" event', function(done){
            var calls = 0;
            var errCalls = 0;

            var test = new Runnable('foo', function(done){
              done(new Error('fail'));
              process.nextTick(done);
              done(new Error('fail'));
              process.nextTick(done);
              process.nextTick(done);
            });

            test.on('error', function(err){
              ++errCalls;
              err.message.should.equal('done() called multiple times');
              calls.should.equal(1);
              errCalls.should.equal(1);
              done();
            });

            test.run(function(){
              ++calls;
            });
          })
        })
\end{lstlisting}
\caption{A failing mocha ``jsapi'' test}
\label{fig:failing-test}
\end{figure*}

The test, shown in Figure~\ref{fig:failing-test}, defines a
part of the behavior of mocha's asynchronous test runner. The test 
asserts that when two asynchronous error events are triggered, only one
is delivered to the error callback. This is important in tests that
allocate resources like database connections, which need to be released
exactly once.

In addition to testing that only a single error event is delivered,
the version 1.0.0 test also tests {\em which} error event is
delivered. In particular, line 20 tests that the error message is a
particular string. The behavior changes at version 1.3.0, from
delivering an error created by mocha that indicates improper usage of
its API, to one of the errors passed by the client to the callback.

Is this case a breaking change, a new feature, or something else?
We might consider that the exceptions thrown by mocha to its clients
are part of the interface, and so this should be considered a breaking
change. Because that the test author's intent is unclear, we follow the
test description, which mentions only the number of error events that
should be delivered. Therefore, we considered {\em which} error event
is delivered to be secret information, and removed it from the test
simply by deleting line 20.

\subsubsection{Failure 3: SemVer violation.}
\label{sec:failure3}
{\bf Test description:}
%
{\em Runner .failHook(hoot, err) should emit "end" }

The method {\tt Runner.failHook} calls a failure hook, and after that
hook has run, emits an ``end'' event to allow test clean up to occur. In
version 1.21.5, whether the ``end'' event should be emitted became
configurable by setting the property {\tt suite.bail}. The
description of this test was changed to reflect this:

{\em Runner .failHook(hoot, err) should emit "end" if suite bail is
  true }

The original test fails because the default value of suite.bail is
{\tt false}, so if a client upgrades mocha to version 1.21.5, the
``end'' event will no longer be emitted. This is a breaking change in
a patch version, and thus is a SemVer violation. We note that the {\tt
  Runner.failHook} method is annotated ``@api private'', so this test
may not actually be part of the interface specification.

\subsubsection{Confounding Factors}
We identify three confounding factors with our user study. First, it
is still unclear that the ``jsapi'' tests should be used as mocha's
interface specification. These tests were identified by the mocha
developers as the ``JavaScript API tests'', and among the JavaScript
libraries we considered for our subject program, seemed to be the most
reasonable test suite to use as a specification. We assume that what
the mocha developers mean by ``API'' is the interface between mocha and
its client. However, this may not always be the case. In practice, this
test suite tests internal interfaces as well as the external
client-facing interface.

A second confounding factor is the small sample size of our study:
only refactored one test suite version of one library. The scope was
limited by the effort required to get good cross-version test
results. Given more time, we would like to examine and refactor more
cross-version testing failures from Mocha other libraries.

A third confounding factor is that the quality of Mocha's test suite
is likely to be quite good compared with the average JavaScript test
suite. This is to be expected, since Mocha itself is a test suite, so
its developers would be expected to be more ``pro-test'' than the
average developer. For this reason, Mocha's test suite may not be
representative of other test suites.

