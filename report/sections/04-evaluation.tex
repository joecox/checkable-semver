\section{Evaluation}

\begin{figure}
\centering
\includegraphics[width=\linewidth]{wisdom_of_the_ancients}
\caption{Violations per version}
\end{figure}

In this section, we report on our evaluation of the practicality of
our discipline. 

\subsection{Quantitative Evaluation}

Our first evaluation is a quantitative analysis of running our tool on a
an existing repostory. In doing so we hope to answer a question:

{\bf RQ1.} Does our tool detect SemVer violations?

We know from the proof that if the interface is fully specified, we can
truthfully detect SemVer violations. The statement does contain 
inherent confounding factors, are test expressive enough to detect
violations, and are tests too strict to specify an interface.

The first confounding factor can easily be related to the problem of
test coverage. Test can be too specific to correctly cover all features
of the program. It is therefor reasonable to ask if an incomplete
specification still can report errors. We have decided to test our tool
on an existing library, mocha\cite{mocha}. Mocha is an testing library,
and was ideal because it had many versions, and was well tested. Mocha
contained 2 interesting test targets;``test-all'', which runs all tests
, and ``test-jsapi'' which tests an internal API\@. These tests were of
cause not designed to follow our discipline, but it enable to answer if
we are able to find any violations at all, assuming that they were. We
ran the tool for each release in the library and collect the breaking
change and added feature violations for each release. For each version
we report the number violated tests, If one test is in multiple versions
it might be counted multiple times, but that also emphasises the
stability of the test.


The second confounding factor, whether too many violations would be
reported, is the most controversial. Many developers has claimed that
merely adhering to SemVer given their internal definition of the 
interface would force them to bump the mayor version every
update\cite{backbone-2888,crawford-not-semver}. This problem could be
more pronounced, if the interface was clearly specified using tests,
and SemVer was checked with a non-compromising tool. To address this
factor, we will simulate running the bump tool for each release in the 
history of the mocha tool. The new version history would then be a
worst-case scenario, where the developers did not expect their tests
describe their interface and was not warned about violations before
releasing.

\subsection{User Study Evaluation}
Our second evaluation is a user study of the efficacy of our
approach. We focus on 

{\bf RQ2.} Without any assumptions about the test suite, does our tool
detect SemVer violations?

{\bf RQ3.} How difficult is it to adopt our discipline of writing API
tests?

To answer {\bf RQ2}, we manually inspected the {\em possible\/} SemVer
violations reported by our tool, in order to determine whether any of
them represent true violations. In particular, we ask whether the test
failure was due to an illegal change in the interface (in which case
the failure is a true violation), due to non-adherence to our
discipline, or for some other reason.

While reviewing the violations for {\bf RQ2}, we refactored failing
tests to adhere to our discipline whenever possible. To answer {\bf
  RQ3}, we recorded the time required for the refactoring. We also
looked for any inherent limitations of our approach that may make
adherence more difficult.

This study was conducted by the authors, who had no prior knowledge of
the internals of Mocha. Lacking ground truth for what the ``true''
interface is, we consulted the Mocha documentation, source code and
comments, and version history to make the best determination
possible. Whenever multiple interpretations were possible, we gave
Mocha developers the benefit of the doubt, choosing the interpretation
that would minimize SemVer violations.

\subsection{Other stuff we learned}
\todo{Where to put this? Approach?}
\begin{itemize}
\item A library should upper-bound their dependencies to stay within
  the current major version, since updrading to the next major version
  of a dependency may introduce a breaking change, which in turn could
  cause a breaking change for the library itself. We found this
  problem with mocha, and had to modify its {\tt package.json} file to
  correct it.
\item NPM packages specify a set of {\tt devDependencies} in the {\tt
    package.json}, which are the dependencies needed by the test suite
  but not the library itself. When cross-version testing, it is
  important to use the {\tt devDependencies} of the tests, not those
  of the library. Therefore, we found that we had to modify {\tt
    package.json} of the library each time we ran a cross-version
  test.
\end{itemize}
